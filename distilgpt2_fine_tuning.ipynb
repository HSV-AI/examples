{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Fine-Tune DistilGPT2 and Generate Text",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HSV-AI/examples/blob/master/distilgpt2_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C9zObeOoEoN"
      },
      "source": [
        "\n",
        "Here is a tutorial about generating text using a SOTA inspired language generation model, distilgpt2. This model lighter in weight and faster in language generation than the original OpenAI GPT2. Using this tutorial, you can train a language generation model which can generate text for any subject in English. Here, we will generate movie reviews by fine-tuning distilgpt2 on a sample of IMDB movie reviews.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW8OfkKEhPPu"
      },
      "source": [
        "Click on the link below and a file will be downloaded containing IMDB sample dataset of 1000 samples\n",
        "\n",
        "http://files.fast.ai/data/examples/imdb_sample.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1bteQwqhsUf"
      },
      "source": [
        "Upload this file in this colab notebook using the upload button on the top left "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUXGPS5Li-E4"
      },
      "source": [
        "Store the reviews in a txt file where each line of txt file is a single review "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LotaG9qgZmHy"
      },
      "source": [
        "file_name = 'testing.txt'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KwFvrFRjOwo"
      },
      "source": [
        "Now, let's come to Transformers by Huggingface, and unleash the Transformers (Autobots... just kidding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkocIBHfaZul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32cab39f-b7cd-4f88-b4ea-e0bd05f045bb"
      },
      "source": [
        "!pip install transformers\n",
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 59.4 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 50.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 7.1 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 61.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.12.1 transformers-4.18.0\n",
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 94922, done.\u001b[K\n",
            "remote: Total 94922 (delta 0), reused 0 (delta 0), pack-reused 94922\u001b[K\n",
            "Receiving objects: 100% (94922/94922), 85.55 MiB | 28.14 MiB/s, done.\n",
            "Resolving deltas: 100% (69667/69667), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers"
      ],
      "metadata": {
        "id": "Z0hUIzo9_Lvs",
        "outputId": "4a326675-8f14-466a-9ced-48a95b8395ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-tdao3y4v\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-tdao3y4v\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (0.5.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (0.12.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (0.0.49)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (4.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.0.dev0) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.19.0.dev0) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.19.0.dev0) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.19.0.dev0) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.19.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.19.0.dev0) (1.15.0)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.19.0.dev0-py3-none-any.whl size=4042246 sha256=62b9884d61929749b3468c061d682ddbd32754baa1cf26f1db4be06e890edfc1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zu4by0rw/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.18.0\n",
            "    Uninstalling transformers-4.18.0:\n",
            "      Successfully uninstalled transformers-4.18.0\n",
            "Successfully installed transformers-4.19.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbIJfTnDjmG6"
      },
      "source": [
        "Make 2 directories. \n",
        "\n",
        "1) weights - for storing the weights of distilgpt2\n",
        "\n",
        "2) tokenizer - for storing the tokenizer of distilgpt2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhH0xD1-qnh-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21868cc5-65b5-447c-8925-109e7b9d1ce0"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.1.0-py3-none-any.whl (325 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 29.7 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 18.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 30 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████                            | 40 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████                           | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████                          | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 71 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 81 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 92 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 184 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 194 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 204 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 215 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 225 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 235 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 245 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 256 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 266 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 276 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 286 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 296 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 307 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 317 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 325 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 70.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 45.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 58.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.5.1)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 44.9 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 56.1 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 56.8 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.1.0 frozenlist-1.3.0 fsspec-2022.3.0 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1HiZgs-kFLg"
      },
      "source": [
        "Now, its time for Training (or fine tuning) distilgpt2 with IMDB reviews\n",
        "Given below is a command containing few parameters to help Transformers finetune distilgpt2. now, let's understand what these parameters mean\n",
        "\n",
        "1) output_dir: It is the weights_dir we made where our finetuned model will be stored in the form of checkpoints\n",
        "\n",
        "2) model_name_or_path: It tells the kind of model we are currently dealing with\n",
        "\n",
        "3) per_device_train_batch_size: It tells the batch size for each gpu\n",
        "\n",
        "4) do_train: It tells pytorch to start training mode\n",
        "\n",
        "5) train_file: This is where we give the input text data \n",
        "\n",
        "6) num_train_epochs: Number of epochs for finetuning\n",
        "\n",
        "\n",
        "Now, let the training begin..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_rFZRe_2KAW"
      },
      "source": [
        "weights_dir = \"output\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42gszj3gkUU1"
      },
      "source": [
        "cmd = '''\n",
        "python transformers/examples/pytorch/language-modeling/run_clm.py \\\n",
        "    --model_name_or_path distilgpt2 \\\n",
        "    --train_file {0} \\\n",
        "    --do_train \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --per_device_train_batch_size 2 \\\n",
        "    --output_dir {1}\n",
        "'''.format(file_name, weights_dir)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQKT9jlOcnjY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57b49cd3-65be-4fd7-9c46-85c714305ffa"
      },
      "source": [
        "!{cmd}"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "04/30/2022 19:45:18 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "04/30/2022 19:45:18 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=output/runs/Apr30_19-45-18_6abea799b1e2,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=output,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=2,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=output,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "04/30/2022 19:45:18 - WARNING - datasets.builder - Using custom data configuration default-c92a76cbd8b6982f\n",
            "04/30/2022 19:45:18 - INFO - datasets.builder - Generating dataset text (/root/.cache/huggingface/datasets/text/default-c92a76cbd8b6982f/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8)\n",
            "Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-c92a76cbd8b6982f/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8...\n",
            "\rDownloading data files:   0% 0/1 [00:00<?, ?it/s]\rDownloading data files: 100% 1/1 [00:00<00:00, 8035.07it/s]\n",
            "04/30/2022 19:45:18 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "04/30/2022 19:45:18 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 848.19it/s]\n",
            "04/30/2022 19:45:18 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "04/30/2022 19:45:18 - INFO - datasets.builder - Generating train split\n",
            "04/30/2022 19:45:19 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-c92a76cbd8b6982f/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 200.89it/s]\n",
            "04/30/2022 19:45:19 - WARNING - datasets.builder - Using custom data configuration default-c92a76cbd8b6982f\n",
            "04/30/2022 19:45:19 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "04/30/2022 19:45:19 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-c92a76cbd8b6982f/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
            "04/30/2022 19:45:19 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-c92a76cbd8b6982f/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8)\n",
            "04/30/2022 19:45:19 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-c92a76cbd8b6982f/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
            "04/30/2022 19:45:19 - WARNING - datasets.builder - Using custom data configuration default-c92a76cbd8b6982f\n",
            "04/30/2022 19:45:19 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "04/30/2022 19:45:19 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-c92a76cbd8b6982f/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
            "04/30/2022 19:45:19 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-c92a76cbd8b6982f/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8)\n",
            "04/30/2022 19:45:19 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-c92a76cbd8b6982f/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
            "[INFO|hub.py:583] 2022-04-30 19:45:19,883 >> https://huggingface.co/distilgpt2/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp4sg8ehjy\n",
            "Downloading: 100% 762/762 [00:00<00:00, 992kB/s]\n",
            "[INFO|hub.py:587] 2022-04-30 19:45:19,986 >> storing https://huggingface.co/distilgpt2/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "[INFO|hub.py:595] 2022-04-30 19:45:19,987 >> creating metadata file for /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "[INFO|configuration_utils.py:659] 2022-04-30 19:45:19,987 >> loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "[INFO|configuration_utils.py:704] 2022-04-30 19:45:19,988 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"distilgpt2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.19.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:370] 2022-04-30 19:45:20,094 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:659] 2022-04-30 19:45:20,189 >> loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "[INFO|configuration_utils.py:704] 2022-04-30 19:45:20,190 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"distilgpt2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.19.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|hub.py:583] 2022-04-30 19:45:20,390 >> https://huggingface.co/distilgpt2/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpvdfccvsg\n",
            "Downloading: 100% 0.99M/0.99M [00:00<00:00, 9.06MB/s]\n",
            "[INFO|hub.py:587] 2022-04-30 19:45:20,648 >> storing https://huggingface.co/distilgpt2/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|hub.py:595] 2022-04-30 19:45:20,648 >> creating metadata file for /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|hub.py:583] 2022-04-30 19:45:20,750 >> https://huggingface.co/distilgpt2/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpsl58oawd\n",
            "Downloading: 100% 446k/446k [00:00<00:00, 4.64MB/s]\n",
            "[INFO|hub.py:587] 2022-04-30 19:45:20,967 >> storing https://huggingface.co/distilgpt2/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|hub.py:595] 2022-04-30 19:45:20,967 >> creating metadata file for /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|hub.py:583] 2022-04-30 19:45:21,067 >> https://huggingface.co/distilgpt2/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpywyazhgq\n",
            "Downloading: 100% 1.29M/1.29M [00:00<00:00, 9.95MB/s]\n",
            "[INFO|hub.py:587] 2022-04-30 19:45:21,329 >> storing https://huggingface.co/distilgpt2/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|hub.py:595] 2022-04-30 19:45:21,329 >> creating metadata file for /root/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-04-30 19:45:21,635 >> loading file https://huggingface.co/distilgpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/55051ac97dcc32f0a736d21a32a4d42b0d9b90f117ca7c38e65038b04bd5c3f5.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-04-30 19:45:21,635 >> loading file https://huggingface.co/distilgpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/9dfb299b74cdf7601ba7cd3a8073dbdac351caec0ed7ab5849b098b3c8ae3d57.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-04-30 19:45:21,635 >> loading file https://huggingface.co/distilgpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/accb287b5a5396b2597382916b6cc939fdab1366e89475a92338d3971b3d02b7.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-04-30 19:45:21,635 >> loading file https://huggingface.co/distilgpt2/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-04-30 19:45:21,635 >> loading file https://huggingface.co/distilgpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-04-30 19:45:21,635 >> loading file https://huggingface.co/distilgpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:659] 2022-04-30 19:45:21,732 >> loading configuration file https://huggingface.co/distilgpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/f985248d2791fcff97732e4ee263617adec1edb5429a2b8421734c6d14e39bee.422318838d1ec4e061efb4ea29671cb2a044e244dc69229682bebd7cacc81631\n",
            "[INFO|configuration_utils.py:704] 2022-04-30 19:45:21,733 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"distilgpt2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.19.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|hub.py:583] 2022-04-30 19:45:21,908 >> https://huggingface.co/distilgpt2/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_bm7i2yu\n",
            "Downloading: 100% 336M/336M [00:07<00:00, 44.6MB/s]\n",
            "[INFO|hub.py:587] 2022-04-30 19:45:30,162 >> storing https://huggingface.co/distilgpt2/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba\n",
            "[INFO|hub.py:595] 2022-04-30 19:45:30,162 >> creating metadata file for /root/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba\n",
            "[INFO|modeling_utils.py:1880] 2022-04-30 19:45:30,162 >> loading weights file https://huggingface.co/distilgpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/43a212e83e76bcb07f45be584cf100676bdbbbe9c13f9e5c1c050049143a832f.a83d881ec4d624fd4b5826dd026e315246c48c67504ff91c0500570e291a54ba\n",
            "[INFO|modeling_utils.py:2190] 2022-04-30 19:45:31,366 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:2199] 2022-04-30 19:45:31,367 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Running tokenizer on dataset:   0% 0/240 [00:00<?, ?ba/s][WARNING|tokenization_utils_base.py:3394] 2022-04-30 19:45:31,495 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1025 > 1024). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|run_clm.py:393] 2022-04-30 19:45:31,495 >> ^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits before being passed to the model.\n",
            "04/30/2022 19:45:31 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-c92a76cbd8b6982f/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-7b926b86a6ff53b6.arrow\n",
            "Running tokenizer on dataset: 100% 240/240 [00:23<00:00, 10.08ba/s]\n",
            "Running tokenizer on dataset:   0% 0/13 [00:00<?, ?ba/s]04/30/2022 19:45:55 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-c92a76cbd8b6982f/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-2cf9853f9be301ad.arrow\n",
            "Running tokenizer on dataset: 100% 13/13 [00:01<00:00, 10.20ba/s]\n",
            "Grouping texts in chunks of 1024:   0% 0/240 [00:00<?, ?ba/s]04/30/2022 19:45:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-c92a76cbd8b6982f/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-6d18926161be9741.arrow\n",
            "Grouping texts in chunks of 1024: 100% 240/240 [00:11<00:00, 21.71ba/s]\n",
            "Grouping texts in chunks of 1024:   0% 0/13 [00:00<?, ?ba/s]04/30/2022 19:46:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-c92a76cbd8b6982f/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-38f6bf2102b69a77.arrow\n",
            "Grouping texts in chunks of 1024: 100% 13/13 [00:00<00:00, 18.11ba/s]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1317] 2022-04-30 19:46:19,547 >> ***** Running training *****\n",
            "[INFO|trainer.py:1318] 2022-04-30 19:46:19,547 >>   Num examples = 10417\n",
            "[INFO|trainer.py:1319] 2022-04-30 19:46:19,547 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:1320] 2022-04-30 19:46:19,547 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:1321] 2022-04-30 19:46:19,547 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "[INFO|trainer.py:1322] 2022-04-30 19:46:19,548 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1323] 2022-04-30 19:46:19,548 >>   Total optimization steps = 5209\n",
            "{'loss': 3.5477, 'learning_rate': 4.520061432136687e-05, 'epoch': 0.1}\n",
            " 10% 500/5209 [03:27<33:36,  2.34it/s][INFO|trainer.py:2193] 2022-04-30 19:49:47,124 >> Saving model checkpoint to output/checkpoint-500\n",
            "[INFO|configuration_utils.py:446] 2022-04-30 19:49:47,125 >> Configuration saved in output/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1469] 2022-04-30 19:49:47,906 >> Model weights saved in output/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2090] 2022-04-30 19:49:47,906 >> tokenizer config file saved in output/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2096] 2022-04-30 19:49:47,907 >> Special tokens file saved in output/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 3.3988, 'learning_rate': 4.040122864273373e-05, 'epoch': 0.19}\n",
            " 19% 1000/5209 [07:06<30:09,  2.33it/s][INFO|trainer.py:2193] 2022-04-30 19:53:25,880 >> Saving model checkpoint to output/checkpoint-1000\n",
            "[INFO|configuration_utils.py:446] 2022-04-30 19:53:25,881 >> Configuration saved in output/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1469] 2022-04-30 19:53:26,728 >> Model weights saved in output/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2090] 2022-04-30 19:53:26,728 >> tokenizer config file saved in output/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2096] 2022-04-30 19:53:26,729 >> Special tokens file saved in output/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 3.3231, 'learning_rate': 3.5601842964100595e-05, 'epoch': 0.29}\n",
            " 29% 1500/5209 [10:45<26:38,  2.32it/s][INFO|trainer.py:2193] 2022-04-30 19:57:04,842 >> Saving model checkpoint to output/checkpoint-1500\n",
            "[INFO|configuration_utils.py:446] 2022-04-30 19:57:04,843 >> Configuration saved in output/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:1469] 2022-04-30 19:57:05,608 >> Model weights saved in output/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2090] 2022-04-30 19:57:05,609 >> tokenizer config file saved in output/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2096] 2022-04-30 19:57:05,609 >> Special tokens file saved in output/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 3.2635, 'learning_rate': 3.080245728546746e-05, 'epoch': 0.38}\n",
            " 38% 2000/5209 [14:24<22:59,  2.33it/s][INFO|trainer.py:2193] 2022-04-30 20:00:43,791 >> Saving model checkpoint to output/checkpoint-2000\n",
            "[INFO|configuration_utils.py:446] 2022-04-30 20:00:43,792 >> Configuration saved in output/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:1469] 2022-04-30 20:00:44,535 >> Model weights saved in output/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2090] 2022-04-30 20:00:44,535 >> tokenizer config file saved in output/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2096] 2022-04-30 20:00:44,535 >> Special tokens file saved in output/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 3.2204, 'learning_rate': 2.6003071606834328e-05, 'epoch': 0.48}\n",
            " 48% 2500/5209 [18:03<19:27,  2.32it/s][INFO|trainer.py:2193] 2022-04-30 20:04:22,760 >> Saving model checkpoint to output/checkpoint-2500\n",
            "[INFO|configuration_utils.py:446] 2022-04-30 20:04:22,761 >> Configuration saved in output/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:1469] 2022-04-30 20:04:23,558 >> Model weights saved in output/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2090] 2022-04-30 20:04:23,559 >> tokenizer config file saved in output/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2096] 2022-04-30 20:04:23,559 >> Special tokens file saved in output/checkpoint-2500/special_tokens_map.json\n",
            "{'loss': 3.2137, 'learning_rate': 2.120368592820119e-05, 'epoch': 0.58}\n",
            " 58% 3000/5209 [21:42<15:52,  2.32it/s][INFO|trainer.py:2193] 2022-04-30 20:08:01,933 >> Saving model checkpoint to output/checkpoint-3000\n",
            "[INFO|configuration_utils.py:446] 2022-04-30 20:08:01,934 >> Configuration saved in output/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:1469] 2022-04-30 20:08:02,757 >> Model weights saved in output/checkpoint-3000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2090] 2022-04-30 20:08:02,757 >> tokenizer config file saved in output/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2096] 2022-04-30 20:08:02,758 >> Special tokens file saved in output/checkpoint-3000/special_tokens_map.json\n",
            "{'loss': 3.2011, 'learning_rate': 1.6404300249568054e-05, 'epoch': 0.67}\n",
            " 67% 3500/5209 [25:21<12:16,  2.32it/s][INFO|trainer.py:2193] 2022-04-30 20:11:41,119 >> Saving model checkpoint to output/checkpoint-3500\n",
            "[INFO|configuration_utils.py:446] 2022-04-30 20:11:41,120 >> Configuration saved in output/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:1469] 2022-04-30 20:11:41,939 >> Model weights saved in output/checkpoint-3500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2090] 2022-04-30 20:11:41,939 >> tokenizer config file saved in output/checkpoint-3500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2096] 2022-04-30 20:11:41,939 >> Special tokens file saved in output/checkpoint-3500/special_tokens_map.json\n",
            "{'loss': 3.1578, 'learning_rate': 1.160491457093492e-05, 'epoch': 0.77}\n",
            " 77% 4000/5209 [29:00<08:39,  2.33it/s][INFO|trainer.py:2193] 2022-04-30 20:15:20,172 >> Saving model checkpoint to output/checkpoint-4000\n",
            "[INFO|configuration_utils.py:446] 2022-04-30 20:15:20,173 >> Configuration saved in output/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:1469] 2022-04-30 20:15:20,966 >> Model weights saved in output/checkpoint-4000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2090] 2022-04-30 20:15:20,966 >> tokenizer config file saved in output/checkpoint-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2096] 2022-04-30 20:15:20,967 >> Special tokens file saved in output/checkpoint-4000/special_tokens_map.json\n",
            "{'loss': 3.1515, 'learning_rate': 6.805528892301785e-06, 'epoch': 0.86}\n",
            " 86% 4500/5209 [32:39<05:06,  2.32it/s][INFO|trainer.py:2193] 2022-04-30 20:18:59,159 >> Saving model checkpoint to output/checkpoint-4500\n",
            "[INFO|configuration_utils.py:446] 2022-04-30 20:18:59,160 >> Configuration saved in output/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:1469] 2022-04-30 20:19:00,021 >> Model weights saved in output/checkpoint-4500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2090] 2022-04-30 20:19:00,022 >> tokenizer config file saved in output/checkpoint-4500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2096] 2022-04-30 20:19:00,022 >> Special tokens file saved in output/checkpoint-4500/special_tokens_map.json\n",
            "{'loss': 3.1643, 'learning_rate': 2.0061432136686505e-06, 'epoch': 0.96}\n",
            " 96% 5000/5209 [36:18<01:30,  2.32it/s][INFO|trainer.py:2193] 2022-04-30 20:22:38,198 >> Saving model checkpoint to output/checkpoint-5000\n",
            "[INFO|configuration_utils.py:446] 2022-04-30 20:22:38,200 >> Configuration saved in output/checkpoint-5000/config.json\n",
            "[INFO|modeling_utils.py:1469] 2022-04-30 20:22:39,108 >> Model weights saved in output/checkpoint-5000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2090] 2022-04-30 20:22:39,109 >> tokenizer config file saved in output/checkpoint-5000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2096] 2022-04-30 20:22:39,109 >> Special tokens file saved in output/checkpoint-5000/special_tokens_map.json\n",
            "100% 5209/5209 [37:52<00:00,  2.69it/s][INFO|trainer.py:1557] 2022-04-30 20:24:11,571 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2272.0456, 'train_samples_per_second': 4.585, 'train_steps_per_second': 2.293, 'train_loss': 3.2603515226613493, 'epoch': 1.0}\n",
            "100% 5209/5209 [37:52<00:00,  2.29it/s]\n",
            "[INFO|trainer.py:2193] 2022-04-30 20:24:11,595 >> Saving model checkpoint to output\n",
            "[INFO|configuration_utils.py:446] 2022-04-30 20:24:11,596 >> Configuration saved in output/config.json\n",
            "[INFO|modeling_utils.py:1469] 2022-04-30 20:24:12,568 >> Model weights saved in output/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2090] 2022-04-30 20:24:12,569 >> tokenizer config file saved in output/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2096] 2022-04-30 20:24:12,569 >> Special tokens file saved in output/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  train_loss               =     3.2604\n",
            "  train_runtime            = 0:37:52.04\n",
            "  train_samples            =      10417\n",
            "  train_samples_per_second =      4.585\n",
            "  train_steps_per_second   =      2.293\n",
            "[INFO|modelcard.py:460] 2022-04-30 20:24:12,875 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKl2zr2Gm2Db"
      },
      "source": [
        "Although, Huggingface provides a run_generation.py file for language generation. Running it from a command (as it takes the input), makes it load the model and the tokenizer everytime you run the file which slows downs generation. To reduce the I/O overhead, I have restructured the run_generation.py file in the following code which only loads the model and tokenizer once in a model and a tokenizer object and we can use these objects to generate text over and over again"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# may be able to use     --save_steps 100  to save space\n",
        "\n",
        "cmd = '''\n",
        "python transformers/examples/pytorch/language-modeling/run_clm.py \\\n",
        "    --model_name_or_path=output/checkpoint-5000 \\\n",
        "    --train_file {0} \\\n",
        "    --do_train \\\n",
        "    --save_steps 10000 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --per_device_train_batch_size 2 \\\n",
        "    --output_dir {1}\n",
        "'''.format(file_name, weights_dir)\n",
        "\n",
        "!{cmd}"
      ],
      "metadata": {
        "id": "ww1RNZwlJIuo",
        "outputId": "3547de08-be58-4ce2-a0f8-157100759f38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "04/30/2022 21:19:19 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "04/30/2022 21:19:19 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=output/runs/Apr30_21-19-19_6abea799b1e2,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=output,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=2,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=output,\n",
            "save_on_each_node=False,\n",
            "save_steps=10000,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "04/30/2022 21:19:19 - WARNING - datasets.builder - Using custom data configuration default-c92a76cbd8b6982f\n",
            "04/30/2022 21:19:19 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "04/30/2022 21:19:19 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-c92a76cbd8b6982f/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
            "04/30/2022 21:19:19 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-c92a76cbd8b6982f/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8)\n",
            "04/30/2022 21:19:19 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-c92a76cbd8b6982f/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
            "100% 1/1 [00:00<00:00,  8.71it/s]\n",
            "04/30/2022 21:19:19 - WARNING - datasets.builder - Using custom data configuration default-c92a76cbd8b6982f\n",
            "04/30/2022 21:19:19 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "04/30/2022 21:19:19 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-c92a76cbd8b6982f/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
            "04/30/2022 21:19:19 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-c92a76cbd8b6982f/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8)\n",
            "04/30/2022 21:19:19 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-c92a76cbd8b6982f/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
            "04/30/2022 21:19:19 - WARNING - datasets.builder - Using custom data configuration default-c92a76cbd8b6982f\n",
            "04/30/2022 21:19:19 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "04/30/2022 21:19:19 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-c92a76cbd8b6982f/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
            "04/30/2022 21:19:19 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-c92a76cbd8b6982f/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8)\n",
            "04/30/2022 21:19:19 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-c92a76cbd8b6982f/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
            "[INFO|configuration_utils.py:657] 2022-04-30 21:19:19,930 >> loading configuration file output/checkpoint-5000/config.json\n",
            "[INFO|configuration_utils.py:704] 2022-04-30 21:19:19,931 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"output/checkpoint-5000\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.19.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1702] 2022-04-30 21:19:19,934 >> Didn't find file output/checkpoint-5000/added_tokens.json. We won't load it.\n",
            "[INFO|tokenization_utils_base.py:1780] 2022-04-30 21:19:19,935 >> loading file output/checkpoint-5000/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1780] 2022-04-30 21:19:19,935 >> loading file output/checkpoint-5000/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1780] 2022-04-30 21:19:19,935 >> loading file output/checkpoint-5000/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1780] 2022-04-30 21:19:19,935 >> loading file None\n",
            "[INFO|tokenization_utils_base.py:1780] 2022-04-30 21:19:19,935 >> loading file output/checkpoint-5000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1780] 2022-04-30 21:19:19,935 >> loading file output/checkpoint-5000/tokenizer_config.json\n",
            "[INFO|modeling_utils.py:1878] 2022-04-30 21:19:20,030 >> loading weights file output/checkpoint-5000/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:2190] 2022-04-30 21:19:21,294 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:2199] 2022-04-30 21:19:21,294 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at output/checkpoint-5000.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "04/30/2022 21:19:21 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-c92a76cbd8b6982f/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-dd42a7c68e2a1a4d.arrow\n",
            "Running tokenizer on dataset:   0% 0/13 [00:00<?, ?ba/s][WARNING|tokenization_utils_base.py:3394] 2022-04-30 21:19:21,625 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1382 > 1024). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|run_clm.py:393] 2022-04-30 21:19:21,625 >> ^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits before being passed to the model.\n",
            "04/30/2022 21:19:21 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-c92a76cbd8b6982f/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-bd771f9864843e36.arrow\n",
            "Running tokenizer on dataset: 100% 13/13 [00:01<00:00,  9.08ba/s]\n",
            "04/30/2022 21:19:22 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-c92a76cbd8b6982f/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-42c1e7a9db5014e5.arrow\n",
            "Grouping texts in chunks of 1024:   0% 0/13 [00:00<?, ?ba/s]04/30/2022 21:19:23 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-c92a76cbd8b6982f/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-8dc564bd65212b6f.arrow\n",
            "Grouping texts in chunks of 1024: 100% 13/13 [00:00<00:00, 20.92ba/s]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1317] 2022-04-30 21:19:28,084 >> ***** Running training *****\n",
            "[INFO|trainer.py:1318] 2022-04-30 21:19:28,084 >>   Num examples = 10417\n",
            "[INFO|trainer.py:1319] 2022-04-30 21:19:28,084 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:1320] 2022-04-30 21:19:28,084 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:1321] 2022-04-30 21:19:28,084 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "[INFO|trainer.py:1322] 2022-04-30 21:19:28,084 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1323] 2022-04-30 21:19:28,084 >>   Total optimization steps = 5209\n",
            "{'loss': 2.8613, 'learning_rate': 4.520061432136687e-05, 'epoch': 0.1}\n",
            "{'loss': 2.895, 'learning_rate': 4.040122864273373e-05, 'epoch': 0.19}\n",
            "{'loss': 2.9054, 'learning_rate': 3.5601842964100595e-05, 'epoch': 0.29}\n",
            "{'loss': 2.9015, 'learning_rate': 3.080245728546746e-05, 'epoch': 0.38}\n",
            "{'loss': 2.9014, 'learning_rate': 2.6003071606834328e-05, 'epoch': 0.48}\n",
            "{'loss': 2.9431, 'learning_rate': 2.120368592820119e-05, 'epoch': 0.58}\n",
            "{'loss': 2.9591, 'learning_rate': 1.6404300249568054e-05, 'epoch': 0.67}\n",
            "{'loss': 2.9412, 'learning_rate': 1.160491457093492e-05, 'epoch': 0.77}\n",
            "{'loss': 2.9608, 'learning_rate': 6.805528892301785e-06, 'epoch': 0.86}\n",
            "{'loss': 3.0046, 'learning_rate': 2.0061432136686505e-06, 'epoch': 0.96}\n",
            "100% 5209/5209 [37:22<00:00,  2.70it/s][INFO|trainer.py:1557] 2022-04-30 21:56:50,633 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2242.5721, 'train_samples_per_second': 4.645, 'train_steps_per_second': 2.323, 'train_loss': 2.9314599215675394, 'epoch': 1.0}\n",
            "100% 5209/5209 [37:22<00:00,  2.32it/s]\n",
            "[INFO|trainer.py:2193] 2022-04-30 21:56:50,658 >> Saving model checkpoint to output\n",
            "[INFO|configuration_utils.py:446] 2022-04-30 21:56:50,659 >> Configuration saved in output/config.json\n",
            "[INFO|modeling_utils.py:1469] 2022-04-30 21:56:51,761 >> Model weights saved in output/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2090] 2022-04-30 21:56:51,762 >> tokenizer config file saved in output/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2096] 2022-04-30 21:56:51,762 >> Special tokens file saved in output/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  train_loss               =     2.9315\n",
            "  train_runtime            = 0:37:22.57\n",
            "  train_samples            =      10417\n",
            "  train_samples_per_second =      4.645\n",
            "  train_steps_per_second   =      2.323\n",
            "[INFO|modelcard.py:460] 2022-04-30 21:56:51,988 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7_-L9gGziiv"
      },
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "def get_model_tokenizer(weights_dir, device = 'cuda'):\n",
        "    print(\"Loading Model ...\")\n",
        "    model = GPT2LMHeadModel.from_pretrained(weights_dir)\n",
        "    model.to('cuda')\n",
        "    print(\"Model Loaded ...\")\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(weights_dir)\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_messages(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt_text,\n",
        "    stop_token,\n",
        "    length,\n",
        "    num_return_sequences,\n",
        "    temperature = 0.7,\n",
        "    k=20,\n",
        "    p=0.9,\n",
        "    repetition_penalty = 1.0,\n",
        "    device = 'cuda'\n",
        "):\n",
        "\n",
        "    MAX_LENGTH = int(10000)\n",
        "    def adjust_length_to_model(length, max_sequence_length):\n",
        "        if length < 0 and max_sequence_length > 0:\n",
        "            length = max_sequence_length\n",
        "        elif 0 < max_sequence_length < length:\n",
        "            length = max_sequence_length  # No generation bigger than model size\n",
        "        elif length < 0:\n",
        "            length = MAX_LENGTH  # avoid infinite loop\n",
        "        return length\n",
        "        \n",
        "    length = adjust_length_to_model(length=length, max_sequence_length=model.config.max_position_embeddings)\n",
        "\n",
        "    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
        "\n",
        "    encoded_prompt = encoded_prompt.to(device)\n",
        "\n",
        "    output_sequences = model.generate(\n",
        "            input_ids=encoded_prompt,\n",
        "            max_length=length + len(encoded_prompt[0]),\n",
        "            temperature=temperature,\n",
        "            top_k=k,\n",
        "            top_p=p,\n",
        "            repetition_penalty=repetition_penalty,\n",
        "            do_sample=True,\n",
        "            num_return_sequences=num_return_sequences,\n",
        "        )\n",
        "\n",
        "    if len(output_sequences.shape) > 2:\n",
        "        output_sequences.squeeze_()\n",
        "\n",
        "    generated_sequences = []\n",
        "\n",
        "    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "        #print(\"=== GENERATED SEQUENCE {} ===\".format(generated_sequence_idx + 1))\n",
        "        generated_sequence = generated_sequence.tolist()\n",
        "\n",
        "        # Decode text\n",
        "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "        # Remove all text after the stop token\n",
        "        text = text[: text.find(stop_token) if stop_token else None]\n",
        "\n",
        "        # Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing\n",
        "        total_sequence = (\n",
        "            prompt_text + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
        "        )\n",
        "\n",
        "        generated_sequences.append(total_sequence)\n",
        "    return generated_sequences\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3fwyPATznLp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68c4885e-02a8-4fd6-be3d-94c915d34c64"
      },
      "source": [
        "\n",
        "model, tokenizer = get_model_tokenizer(weights_dir, device = 'cuda')\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Model ...\n",
            "Model Loaded ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqpCwzTUz0wZ"
      },
      "source": [
        "temperature = 1.0\n",
        "k=400\n",
        "p=0.9\n",
        "repetition_penalty = 1.0\n",
        "num_return_sequences = 5\n",
        "length = 1000\n",
        "stop_token = '|EndOfText|'\n",
        "prompt_text = \"material for weapon\""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQv52LcT0Iyi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca46a41e-c216-42b0-9dc7-682712b546c9"
      },
      "source": [
        "%%time\n",
        "generate_messages(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt_text,\n",
        "    stop_token,\n",
        "    length,\n",
        "    num_return_sequences,\n",
        "    temperature = temperature,\n",
        "    k=k,\n",
        "    p=p,\n",
        "    repetition_penalty = repetition_penalty\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 9.28 s, sys: 4.41 ms, total: 9.28 s\n",
            "Wall time: 9.32 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['material for weapon system architectures.\\n\\t\\t\\t\\tPHASE II: Develop a multi-modal approach to enable demonstration of state of the art weapon systems. Prepare Phase II plans for demonstration with MIL-STD-18485. \\n\\t\\tPHASE III: Support the Army in transitioning the technology and its technology to government commercialization. \\n\\t\\tREFERENCES: 1: Q. Bien, Li, Qiang C. Wang, Xiaolyng Q. Zhang, &quot;Weapon systems: proof-of-concept, proof-of-concept, simulation, and design tools; Proceedings of the 12th Advanced Air Mobility Program, August 2004.2: P. Wang, W. Yan, H. Li, G. Wang, Z. S. Hu, M. L. Ye, and M. Chen. (2012). Modeling of multiple weapons systems with single system. Proceedings of the 7th AIAA Group, Volume A, Issue B, Issue C, Issue C.3: A. A. Chen, Y. Chen, W. Jiang, and J. Yung. (2013). Characterization and integration of multiple capabilities to identify system boundary conditions: DoD’s interest. Proc. SPIE 11010, Vol. 30, Issue 3, pp.2-4, pp.938-6,  https://spie.ieee.org/portfolio/content/dma11010/u0022602575_u0022602575_u0022602575_u00226075_u00226075_u0022602575_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_u00226075_',\n",
              " 'material for weapon systems. Proposed proposals will be accompanied by a detailed description of the technology and performance characteristics of the proposed technology and, the appropriate Phase II plans.\\nPHASE II: The Air Force seeks to develop concepts and technologies that will transition the technology development to Army (AM) ready-to-use, and transitioning the technology for commercial use. Proposed research areas include (but are not limited to) technical analysis, analysis, and validation of the prototype concepts. The technologies developed during this Phase II may be re-evaluated and evaluated to determine the likelihood of commercialization. Projects that involve the Army, Navy, and commercialize the technology will have to wait until a full time development stage for Phase III. Innovative projects of interest include (but are not limited to) the development of weapon systems, to meet a range of military and commercial requirements, new diagnostic testing, and/or detection capabilities for radar, radar, or radar/aircraft communications. These technologies are expected to address challenges such as large, forward facing threats, or detection of various types of laser threats. Of particular interest, will new technologies be considered relevant to this challenge. In the areas of MQ-1 lasers, any proposed solutions must address all types of lasers as defined by MIL-STD-810G lasers, and/or MQ-2 lasers that will be deployed to or in applications with the goal of avoiding high radiation emissions from those lasers.\\nREFERENCES:\\n\\n\\tKrisch, J., Vitek, A., &amp; Muthitov, A. (2015). Laser effects on the composition of laser-resistant glass arrays: the impact of fiber and mica beam coatings on the optical performance, N. Opt. Rev. Int. 53, 52 (1): 270-271.\\n\\tMuthitov, A. (2016). Laser effects on optical performance of laser-resistant glass arrays. Ed. AIAA/ASP Optics Journal. 1650 (1): 2-5.\\n\\tJürgenhöl, B., &amp; Gudges, J. (2017). Laser effects on the shape and function of composite glass arrays: a review of the literature. Opt. Rev. Sci. 35, 619-623.\\n\\tChrabank, P., &amp; Borgin, P. (2016). High-density laser absorbing systems for continuous optical operation, N. Opt. Rev. Sci. 30, 270 (1): 755-774.\\n\\tPulkar, P., Ayer, A., &amp; Shiller, A. (2015). Laser effects on mechanical performance, N. Opt. Rev. Soc. A. Med., no. 6: 597-504.\\n\\tSang, J., Fischke, A. J., Schoeps, S., Levensen, M., &amp; Smith, J. (2019).  Mechanical properties of stainless-type composite fiber systems and machining results in high conductivity and/or improved material properties of MQ-1 laser systems. Anal. Optics, Vol. 18, No. 6, pp. 198–219.\\n\\tPulkar, P., Linnström, G., &amp; Carver, S. (2014).  Automotive fiber performance, N. Opt. Sci. 27, 917-919.\\n\\tStuur, P., Vitek, A., and Spies, G. (2009). Lacking the strength of thin flexible glass arrays for multiple-beam diffraction, N. Opt. Rev. Rev. Med. Soc. A. Med., no. 6, pp. 450–453.\\n\\tKrisch, J., Trac-Mulffier, E., Llino, L., Vitek, A., Rumpid, C., and Parhan, P. (2014). The size and power characteristics of H1S‐based multirelled polymer composites. Ed. AIAA AO. Optics., Vol. 15, No. 6, pp. 2176-2197.\\n\\tDedueng, A., and Mandehan, J. (2014). Low-energy thin films, integrated beam absorbing systems, J. Opt. Rev. Int. 65, 201(2): 575-578.\\n\\tLatham, J., and Bonenau, J. (2013). Optical and thermal conductivity of thin-film composites using high contrast laser reflectance (i.e., H1S‐based thermoelectric reflectance) laser based laser coated glass arrays. Ed. AIAA Optics., Vol. 34, No. 3, pp. 8911–9014.\\n\\tLee, C., and Martin',\n",
              " 'material for weapon systems or processes.\\n   \\n  \\n    \\n    \\n        NUDL/O3/NINDS.\\n   \\n    \\n     \\n        6. Non-optical design and engineering of infrared, or pneumatic actuators to provide visible, infrared, or pulsed information for threat and reconnaissance.\\n    \\n    \\n    \\n    \\n         7. Distinguishing material/technology from solid and/or solid-state plasma plasma.\\n    \\n   \\n    \\n    \\n    \\n    \\n       8.  NVDM for analysis of particles in plasma.\\n    \\n    \\n    \\n     \\n       9. Non-destructive tools to analyze material, process, or any other sensitive material or system of its function to detect radiation induced by solid and/or solid particles in the plasma or plasma.\\n    \\n    \\n    \\n     \\n        10. Measurement of particle and plasma material with temperature and/or pulse density in relevant quantities, including 0.5 MeV/m2.\\n    \\n   \\n       \\n       11. Preclinical or experimental test or device for the detection of gamma rays in plasma.\\n    \\n    \\n    \\n     \\n     \\n        12. Biomedical engineering efforts for early detection of gamma rays in plasma with conventional detectors.\\n    \\n    \\n     \\n    \\n     \\n         13. High-performance, low-cost optical and mathematical instrumentation.\\n    \\n   \\n    \\n     \\n    \\n    \\n       14. General measurements of nuclei in plasma with temperature, pulse, pulse, pulse, pulse, pulse or pulse width.\\n    \\n    \\n    \\n    \\n    \\n        15. Dose and oxygenation, molecular or electron-ion isotope (NII) separation of small particles to achieve large-scale, high-resolution high resolution neutron particle analysis.\\n    \\n    \\n    \\n    \\n    \\n       16. High-efficiency, high performance neutron particle characterization, simulation and characterization of superconducting magnetic materials and materials.\\n    \\n    \\n    \\n    \\n    \\n    \\n        17. Electron-ion isotope (EM) particle characterization, analysis, and development of high-precision, high-resolution high-efficiency photon analysis and characterization systems.\\n    \\n   \\n    \\n       18. Polymer-chromosome techniques for detection of polymers in plasma with atomic energies less than 20 MeV/m2.\\n    \\n    \\n    \\n    \\n       20. Electron-ion isotope, characterization and characterization of hybrid components of small particles and their electronic devices, including electron and gamma rays, in low-power ionuclides (LLs).\\n    \\n    \\n    \\n    \\n       21. High-efficiency, high-resolution electron particle characterization, characterization, and development of high-resolution electron-ion isotope characterization methods for high-precision electron particle analysis and characterization.\\n    \\n    \\n    \\n       22. High-efficiency, high-performance electron-ion tests (ESRTs) in materials such as organic/organic compounds and organic composites.\\n    \\n    \\n    \\n     \\n       23. High performance, high-te',\n",
              " 'material for weapon systems, weapon systems, and other systems. Proposals should focus on: \\n\\n\\tImproved and improved beam diagnostics, including improved sensors, and improved ways to define and limit radiation propagation patterns.\\n\\tImproved measurement and measurement methods, including those that are based on radiation, and of limited application.\\n\\tImprovements to the SBIR’s associated support system, while addressing the fundamental requirements for radiation and other applications.\\n\\tImproved, personalized, and secure communication with providers, allowing them to collect, analyze, and use existing data or data provided by government agencies.\\n\\tImproved manufacturing of critical sensors and technologies.\\n\\tImproved control of the manufacturing process, including controls on key components and components, and especially a key component implementation.\\n\\tImproved manufacturing system integration with commercial or other industries and commercial use cases.\\n\\tImproved or improved methods for monitoring and remediation of radiation exposures and injuries to sensitive sites, facilities, and commercial sensors.\\n\\tImproved the ability to protect sensitive environmental sources and components from radiation.\\n\\tImproved, more reliable methods for accurately estimating and predicting radiation exposure, including time-domain changes, strain and strain patterns, and changes in the magnitude and magnitude of radiation.\\n\\tBetter support for operations and hazardous materials, including damage to components, or for protection of hazardous materials.\\n\\tImprovements in diagnostic instrumentation, including radiation detection and determination of particulates, particulates, particulates, and contaminants, and their concentration, concentration and distribution.\\n\\tImprovements in calibration, and for improved signal processing and instrumentation.\\n\\tAdvanced and improved assessment and assessment of radiation exposure and wound protection.\\n\\tDevelopment of high-quality, low-cost, low-throughput, robust, accurate, high-quality, accurate, effective, and reliable, and effective diagnostic methods.\\n\\tImproved the ability to predict, detect, monitor, and measure irradiance in space environments, with the development of new methods for radiation monitoring.\\n\\tInnovative, low-cost, and high-precision, intelligent, and accurate diagnostic methods for such analyses and radiation localization, along with improved modeling, modeling, and analysis of biological, chemical, and environmental interactions.\\n\\tImprovement in the transmission of measurements and measurements in space environments, with the development of improved methodologies that enable the rapid and accurate translation of long-lived, radioactive, or radiation data.\\n\\tImproved understanding of radiation exposures and effects on health and safety.\\n\\tImproved integration and validation of advanced and cost-effective diagnostic methods for detection and remediation of radiation hazards.\\n\\tImproved or improved techniques for instrumentation, including measurement and detection of radiation exposure and wound protection.\\n\\tImproved measurement and processing of material, components, and components, and especially a key component implementation.\\n\\tImproved statistical methods that allow rapid and accurate, reproducible, and real-time measurements of radiation exposure and wound protection to laboratory instrumentation.\\n\\tImproved design, in-person, and statistical methods to assess the effectiveness of radiation sensitivity, including the integration of new methods for measuring, measuring, and reporting radiation levels.\\n\\tImproved analytical methods for predicting and addressing radiation exposures and injuries.\\n\\tDevelopments to the Safety, Safety, and Performance of Nuclear and Atmospheric Environment (SPSE) Research Program, (http://science.energy.gov/science/safety/safety-and-performance-of-nuclear-and-aerospace-environment/).\\n\\tImproved development and validation of: \\n\\n\\tC.A. Milbius, A.J. Steppe, and C.E. Morbius, &quot;Developing an environmentally-friendly, cost-effective, cost-effective, cost-effective, automated, cost-effective, cost-effective, scalable, cost-effective, cost-effective.” IEEE, vol. 2, no. 1, pp. 52-65, 2019.\\n\\tPortfolio information for the Small Business Innovation Research Program will be provided by the Small Business Innovation Research Program (STOP) to assist the Small Business Innovation Research Program (SBIR) to develop innovative products to improve safety and performance of nuclear and terrestrial sensors and products, to protect the nation from hazardous materials, industrial waste and homeland security threats. STOP’s focus areas will be on the safety, safety, and performance of nuclear and terrestrial sensors and products, and technologies; the use of products or services as utilities, to enhance or improve safety and effectiveness of nuclear and terrestrial sensors, products or services, in accordance with the specific objective of the STOP. The STOP’s focus areas will be on cost effective, cost effective, cost-effective, cost-effective, cost-effective, cost-effective, cost-effective, cost-effective, cost-effective, cost-effective, cost-effective, cost-effective.\\n\\tPhase I',\n",
              " 'material for weapon systems, systems, or structures in high-performance distributed system architectures and for the detection and mitigation of space-based threats. ']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpXtDNZ4dunQ"
      },
      "source": [
        "!rm -r output/checkpoint-500\n",
        "!rm -r output/checkpoint-1*\n",
        "!rm -r output/checkpoint-2*\n",
        "!rm -r output/checkpoint-3*\n",
        "!rm -r output/checkpoint-4*\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9L6_yoWKLAkE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}